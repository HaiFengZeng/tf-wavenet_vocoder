{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "sys.path.append('../')\n",
    "from wavenet.model_ibab import WaveNetModel\n",
    "from wavenet.ops import mu_law_encode, mu_law_decode\n",
    "from IPython.display import Audio\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_conv(model, input_batch, state_batch, weights):\n",
    "    '''Perform convolution for a single convolutional processing step.'''\n",
    "    \n",
    "    output = tf.matmul(state_batch[0], weights[0])\n",
    "    \n",
    "    i = 0 # This value will be used when filter width == 2\n",
    "    for i in range(1, model.filter_width-1):\n",
    "        output += tf.matmul(state_batch[i], weights[i])\n",
    "    \n",
    "    output += tf.matmul(input_batch, weights[i+1])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def generator_causal_layer(model, input_batch, state_batch):\n",
    "    with tf.name_scope('causal_layer'):\n",
    "        weights_filter = model.variables['causal_layer']['filter']\n",
    "        output = generator_conv(model, input_batch, state_batch, weights_filter)\n",
    "    return output\n",
    "\n",
    "def generator_dilation_layer(model, input_batch, state_batch, layer_index,\n",
    "                              dilation, local_condition_batch, global_condition_batch):\n",
    "    variables = model.variables['dilated_stack'][layer_index]\n",
    "\n",
    "    weights_filter = variables['filter']\n",
    "    weights_gate = variables['gate']\n",
    "    output_filter = generator_conv(model, input_batch, state_batch, weights_filter)\n",
    "    output_gate = generator_conv(model, input_batch, state_batch, weights_gate)\n",
    "\n",
    "    if local_condition_batch is not None:\n",
    "        output_filter += tf.matmul(local_condition_batch, variables['cond_filter'][0, :, :])\n",
    "        output_gate += tf.matmul(local_condition_batch, variables['cond_gate'][0, :, :])        \n",
    "\n",
    "    if global_condition_batch is not None:\n",
    "        global_condition_batch = tf.reshape(global_condition_batch,\n",
    "                                            shape=(1, -1))\n",
    "        output_filter += tf.matmul(global_condition_batch,\n",
    "                                   variables['gc_filtweights'][0, :, :])\n",
    "        output_gate += tf.matmul(global_condition_batch,\n",
    "                                 variables['gc_gateweights'][0, :, :])\n",
    "\n",
    "    if model.use_biases:\n",
    "        output_filter = output_filter + variables['filter_bias']\n",
    "        output_gate = output_gate + variables['gate_bias']\n",
    "\n",
    "    out = tf.tanh(output_filter) * tf.sigmoid(output_gate)\n",
    "\n",
    "    weights_dense = variables['dense']\n",
    "    transformed = tf.matmul(out, weights_dense[0, :, :])\n",
    "    if model.use_biases:\n",
    "        transformed = transformed + variables['dense_bias']\n",
    "\n",
    "    weights_skip = variables['skip']\n",
    "    skip_contribution = tf.matmul(out, weights_skip[0, :, :])\n",
    "    if model.use_biases:\n",
    "        skip_contribution = skip_contribution + variables['skip_bias']\n",
    "\n",
    "    return skip_contribution, input_batch + transformed\n",
    "\n",
    "\n",
    "def create_queue(model, dilation, n_channel, name=None):\n",
    "    shape = (dilation * (model.filter_width - 1), model.batch_size, n_channel)\n",
    "    value = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(value, trainable=False, name=name)\n",
    "\n",
    "def create_q_ops(model):\n",
    "    wrapper = []\n",
    "    q = create_queue(model, 1, model.quantization_channels, \"Q_L0\")\n",
    "\n",
    "    wrapper.append(q)\n",
    "    # Add all defined dilation layers.\n",
    "    with tf.name_scope('dilated_stack'):\n",
    "        for layer_index, dilation in enumerate(model.dilations):\n",
    "            with tf.name_scope('layer{}'.format(layer_index)):\n",
    "                q = create_queue(model, dilation, model.residual_channels, \"Q_L{}\".format(layer_index+1))\n",
    "                wrapper.append(q)\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "def update(q, current_q_idx, dil, filter_width, x):\n",
    "    # dequeue\n",
    "    for i in range(1, filter_width - 1):\n",
    "        q = tf.scatter_update(q, current_q_idx + i - 1, \n",
    "                                 q[current_q_idx + i])  \n",
    "        \n",
    "    # enqueue\n",
    "    q = tf.scatter_update(q, current_q_idx + (filter_width - 2), x)  \n",
    " \n",
    "    return q\n",
    "\n",
    "def create_update_q_ops(model, qs, initial, others, gen_num):\n",
    "    dilation = 1\n",
    "    current_q_idx = 0\n",
    "    \n",
    "    q = qs[0]\n",
    "    q = update(q, current_q_idx, dilation, model.filter_width, \n",
    "                tf.reshape(initial, [model.batch_size, model.quantization_channels], name=\"up_reshape0\"))\n",
    "    qs[0] = q\n",
    "    \n",
    "    for layer_index, dilation in enumerate(model.dilations):\n",
    "        q = qs[layer_index + 1]\n",
    "        current_q_idx = (gen_num % dilation) * (model.filter_width - 1)\n",
    "        q = update(q, current_q_idx, dilation, model.filter_width, \n",
    "                    tf.reshape(others[layer_index], [model.batch_size, model.residual_channels], \n",
    "                               name=\"up_reshape{}\".format(layer_index + 1)))\n",
    "        qs[layer_index + 1] = q\n",
    "    \n",
    "    return qs\n",
    "    \n",
    "def predict(model, qs, x, c, g, gen_num):\n",
    "    outputs = []\n",
    "    layers = []\n",
    "    \n",
    "    q = qs[0]\n",
    "    \n",
    "    dilation = 1\n",
    "    current_q_idx = 0\n",
    "    current_data_idx = current_q_idx + (model.filter_width - 1)\n",
    "    \n",
    "    current_layer = x\n",
    "#     current_layer = tf.Print(current_layer, [current_layer], message=\"current_layer0:\")\n",
    "    current_state = q[current_q_idx:current_data_idx]\n",
    "    layers.append(current_layer)\n",
    "    \n",
    "    current_layer = generator_causal_layer(model, current_layer, current_state)\n",
    "    \n",
    "    # Add all defined dilation layers.\n",
    "    with tf.name_scope('dilated_stack'):\n",
    "        for layer_index, dilation in enumerate(model.dilations):\n",
    "            with tf.name_scope('layer{}'.format(layer_index)):\n",
    "                q = qs[layer_index + 1]\n",
    "                current_q_idx = (gen_num % dilation) * (model.filter_width - 1)\n",
    "                current_data_idx = current_q_idx + (model.filter_width - 1)\n",
    "                layers.append(current_layer)\n",
    "                \n",
    "                current_state = q[current_q_idx:current_data_idx]\n",
    "#                 current_layer = tf.Print(current_layer, [current_layer], \n",
    "#                                          message=\"current_layer{}:\".format(layer_index + 1))\n",
    "                \n",
    "                output, current_layer = generator_dilation_layer(model,\n",
    "                    current_layer, current_state, layer_index, dilation, c, g)\n",
    "                outputs.append(output)\n",
    "                \n",
    "    with tf.name_scope('postprocessing'):\n",
    "        variables = model.variables['postprocessing']\n",
    "        # Perform (+) -> ReLU -> 1x1 conv -> ReLU -> 1x1 conv to\n",
    "        # postprocess the output.\n",
    "        w1 = variables['postprocess1']\n",
    "        w2 = variables['postprocess2']\n",
    "        if model.use_biases:\n",
    "            b1 = variables['postprocess1_bias']\n",
    "            b2 = variables['postprocess2_bias']\n",
    "\n",
    "        # We skip connections from the outputs of each layer, adding them\n",
    "        # all up here.\n",
    "        total = sum(outputs)\n",
    "        transformed1 = tf.nn.relu(total)\n",
    "\n",
    "        conv1 = tf.matmul(transformed1, w1[0, :, :])\n",
    "        if model.use_biases:\n",
    "            conv1 = conv1 + b1\n",
    "        transformed2 = tf.nn.relu(conv1)\n",
    "        conv2 = tf.matmul(transformed2, w2[0, :, :])\n",
    "        if model.use_biases:\n",
    "            conv2 = conv2 + b2    \n",
    "            \n",
    "    return conv2, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dilations: [1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128]\n",
      "created.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 1\n",
    "filter_width = 3\n",
    "n_stack = 2\n",
    "max_dilation = 8\n",
    "dilations = [2 ** i for j in range(n_stack) for i in range(max_dilation)]\n",
    "print(\"dilations:\", dilations)\n",
    "residual_channels, dilation_channels, skip_channels = 256, 256, 256\n",
    "use_biases = True\n",
    "quantization_channels = 256\n",
    "gc_cardinality = None\n",
    "gc_channels = None\n",
    "scalar_input = False\n",
    "initial_filter_width = None\n",
    "\n",
    "net = WaveNetModel(batch_size=batch_size,\n",
    "                        dilations=dilations,\n",
    "                        filter_width=filter_width,\n",
    "                        scalar_input = scalar_input,\n",
    "                        initial_filter_width=32,\n",
    "                        residual_channels=residual_channels,\n",
    "                        dilation_channels=dilation_channels,\n",
    "                        quantization_channels=quantization_channels,\n",
    "                        skip_channels=skip_channels,\n",
    "                        global_condition_channels=gc_channels,\n",
    "                        global_condition_cardinality=gc_cardinality,\n",
    "                        use_biases=use_biases)\n",
    "\n",
    "waveform = tf.placeholder(tf.float32)\n",
    "ml_encoded = mu_law_encode(waveform, quantization_channels)\n",
    "encoded = net._one_hot(ml_encoded)\n",
    "encoded = tf.reshape(encoded, [batch_size, -1, quantization_channels])\n",
    "\n",
    "raw = net._create_network(encoded, None)\n",
    "raw = tf.reshape(raw, [batch_size, -1, quantization_channels])\n",
    "proba = tf.cast(tf.nn.softmax(tf.cast(raw, tf.float64)), tf.float32)\n",
    "\n",
    "# for generation\n",
    "sample_placeholder = tf.placeholder(tf.int32)\n",
    "encoded_sample = net._one_hot(sample_placeholder)\n",
    "encoded_sample = tf.reshape(encoded_sample, [-1, quantization_channels])\n",
    "gen_num = tf.placeholder(tf.int32)\n",
    "qs = create_q_ops(net)\n",
    "\n",
    "next_sample, layers_out = predict(net, qs, encoded_sample, None, None, gen_num)\n",
    "next_sample = tf.reshape(next_sample, [-1, quantization_channels])\n",
    "next_sample = tf.cast(tf.nn.softmax(tf.cast(next_sample, tf.float64)), tf.float32)\n",
    "\n",
    "initial = tf.placeholder(tf.float32)\n",
    "others = tf.placeholder(tf.float32)\n",
    "update_q_ops = create_update_q_ops(net, qs, initial, others, gen_num)\n",
    "\n",
    "var_q = list(filter(lambda var : var.name.split('/')[-1].startswith(\"Q_L\"), \n",
    "                    tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)))\n",
    "print(\"created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 9.298052072525024\n",
      "result: [[225 225 246  57  34 246 225 143 225 225 246]\n",
      " [225 225 225 225 225 225 143 246 202 246 246]]\n",
      "generated samples: [[225 225 246  57  34 246 225 143 225 225 246]\n",
      " [225 225 225 225 225 225 143 246 202 246 246]]\n",
      "difference between result and samples: 0\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for j in range(batch_size):\n",
    "     data.append(np.concatenate([np.zeros(net.receptive_field), np.random.randn(10)]))\n",
    "data = np.vstack(data)\n",
    "\n",
    "sess_config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result, _encoded = sess.run([proba, ml_encoded], \n",
    "                                   feed_dict={waveform:data})\n",
    "    _encoded = _encoded.reshape(batch_size, -1)\n",
    "    result = np.argmax(result, axis=-1)\n",
    "\n",
    "    sess.run(tf.variables_initializer(var_q))\n",
    "    \n",
    "    t = time()\n",
    "    samples= []\n",
    "    for j in range(net.receptive_field-1):\n",
    "        feed_dict = {sample_placeholder:_encoded[:,j], gen_num:j}\n",
    "        prob, _layers = sess.run([next_sample, layers_out], feed_dict=feed_dict)\n",
    "        sess.run(update_q_ops, feed_dict={initial:_layers[0], others:np.array(_layers[1:]), gen_num:j})\n",
    "#         print(\"current_layer:\", prob)\n",
    "\n",
    "    for j in range(net.receptive_field-1, _encoded.shape[-1]):\n",
    "        feed_dict = {sample_placeholder:_encoded[:,j], gen_num:j}\n",
    "        prob, _layers = sess.run([next_sample, layers_out], feed_dict=feed_dict)\n",
    "        sess.run(update_q_ops, feed_dict={initial:_layers[0], others:np.array(_layers[1:]), gen_num:j})\n",
    "#         print(\"current_layer:\", prob)\n",
    "        sample = np.argmax(prob, axis=-1)\n",
    "        samples.append(sample)\n",
    "    samples = np.array(samples).T\n",
    "    print(\"elapsed:\", time()-t)\n",
    "\n",
    "print(\"result:\", result)\n",
    "print(\"generated samples:\", samples)\n",
    "print(\"difference between result and samples:\", np.abs(result-samples).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 109.17055201530457\n",
      "difference between result and samples: 0\n"
     ]
    }
   ],
   "source": [
    "src, _ = librosa.load(\"voice.wav\", sr=16000)\n",
    "n_samples = len(src)\n",
    "src = src.reshape(-1, 1)\n",
    "src = np.pad(src, [[net.receptive_field, 0], [0, 0]],'constant')\n",
    "data = []\n",
    "for j in range(batch_size):\n",
    "     data.append(src)\n",
    "data = np.vstack(data)\n",
    "\n",
    "sess_config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    result, _encoded = sess.run([proba, ml_encoded], feed_dict={waveform:data})\n",
    "    _encoded = _encoded.reshape(batch_size, -1)\n",
    "    result = np.argmax(result, axis=-1)\n",
    "    \n",
    "    sess.run(tf.variables_initializer(var_q))\n",
    "    \n",
    "    t = time()\n",
    "    samples= []\n",
    "    for j in range(net.receptive_field-1):\n",
    "        feed_dict = {sample_placeholder:_encoded[:,j], gen_num:j}\n",
    "        prob, _layers = sess.run([next_sample, layers_out], feed_dict=feed_dict)\n",
    "        sess.run(update_q_ops, feed_dict={initial:_layers[0], others:np.array(_layers[1:]), gen_num:j})\n",
    "#         print(\"current_layer:\", prob)\n",
    "\n",
    "    for j in range(net.receptive_field-1, _encoded.shape[-1]):\n",
    "        feed_dict = {sample_placeholder:_encoded[:,j], gen_num:j}\n",
    "        prob, _layers = sess.run([next_sample, layers_out], feed_dict=feed_dict)\n",
    "        sess.run(update_q_ops, feed_dict={initial:_layers[0], others:np.array(_layers[1:]), gen_num:j})\n",
    "#         print(\"current_layer:\", prob)\n",
    "        sample = np.argmax(prob, axis=-1)\n",
    "        samples.append(sample)\n",
    "    samples = np.array(samples).T\n",
    "    print(\"elapsed:\", time()-t)\n",
    "    \n",
    "print(\"difference between result and samples:\", np.abs(result-samples).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

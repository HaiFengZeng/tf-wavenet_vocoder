{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import time\n",
    "sys.path.append('..')\n",
    "from wavenet.model import WaveNetModel\n",
    "from wavenet.ops import mu_law_decode, mu_law_encode\n",
    "from IPython.display import Audio\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import librosa\n",
    "import pysptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_shape(x):\n",
    "    return list(map(int, x.get_shape()))\n",
    "\n",
    "def discretized_mix_logistic_loss(x,l,sum_all=True):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    xs = int_shape(x) # true image (i.e. labels) to regress to, e.g. (B,32,32,3)\n",
    "    ls = int_shape(l) # predicted distribution, e.g. (B,32,32,100)\n",
    "    nr_mix = int(ls[-1] / 10) # here and below: unpacking the params of the mixture of logistics\n",
    "    logit_probs = l[:,:,:,:nr_mix]\n",
    "    l = tf.reshape(l[:,:,:,nr_mix:], xs + [nr_mix*3])\n",
    "    means = l[:,:,:,:,:nr_mix]\n",
    "    log_scales = tf.maximum(l[:,:,:,:,nr_mix:2*nr_mix], -7.)\n",
    "    coeffs = tf.nn.tanh(l[:,:,:,:,2*nr_mix:3*nr_mix])\n",
    "    x = tf.reshape(x, xs + [1]) + tf.zeros(xs + [nr_mix]) # here and below: getting the means and adjusting them based on preceding sub-pixels\n",
    "    m2 = tf.reshape(means[:,:,:,1,:] + coeffs[:, :, :, 0, :] * x[:, :, :, 0, :], [xs[0],xs[1],xs[2],1,nr_mix])\n",
    "    m3 = tf.reshape(means[:, :, :, 2, :] + coeffs[:, :, :, 1, :] * x[:, :, :, 0, :] + coeffs[:, :, :, 2, :] * x[:, :, :, 1, :], [xs[0],xs[1],xs[2],1,nr_mix])\n",
    "    means = tf.concat([tf.reshape(means[:,:,:,0,:], [xs[0],xs[1],xs[2],1,nr_mix]), m2, m3],3)\n",
    "    centered_x = x - means\n",
    "    inv_stdv = tf.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1./255.)\n",
    "    cdf_plus = tf.nn.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1./255.)\n",
    "    cdf_min = tf.nn.sigmoid(min_in)\n",
    "    log_cdf_plus = plus_in - tf.nn.softplus(plus_in) # log probability for edge case of 0 (before scaling)\n",
    "    log_one_minus_cdf_min = -tf.nn.softplus(min_in) # log probability for edge case of 255 (before scaling)\n",
    "    cdf_delta = cdf_plus - cdf_min # probability for all other cases\n",
    "    mid_in = inv_stdv * centered_x\n",
    "    log_pdf_mid = mid_in - log_scales - 2.*tf.nn.softplus(mid_in) # log probability in the center of the bin, to be used in extreme cases (not actually used in our code)\n",
    "\n",
    "    # now select the right output: left edge case, right edge case, normal case, extremely low prob case (doesn't actually happen for us)\n",
    "\n",
    "    # this is what we are really doing, but using the robust version below for extreme cases in other applications and to avoid NaN issue with tf.select()\n",
    "    # log_probs = tf.select(x < -0.999, log_cdf_plus, tf.select(x > 0.999, log_one_minus_cdf_min, tf.log(cdf_delta)))\n",
    "\n",
    "    # robust version, that still works if probabilities are below 1e-5 (which never happens in our code)\n",
    "    # tensorflow backpropagates through tf.select() by multiplying with zero instead of selecting: this requires use to use some ugly tricks to avoid potential NaNs\n",
    "    # the 1e-12 in tf.maximum(cdf_delta, 1e-12) is never actually used as output, it's purely there to get around the tf.select() gradient issue\n",
    "    # if the probability on a sub-pixel is below 1e-5, we use an approximation based on the assumption that the log-density is constant in the bin of the observed sub-pixel value\n",
    "    log_probs = tf.where(x < -0.999, log_cdf_plus, tf.where(x > 0.999, log_one_minus_cdf_min, tf.where(cdf_delta > 1e-5, tf.log(tf.maximum(cdf_delta, 1e-12)), log_pdf_mid - np.log(127.5))))\n",
    "\n",
    "    log_probs = tf.reduce_sum(log_probs,3) + log_prob_from_logits(logit_probs)\n",
    "    if sum_all:\n",
    "        return -tf.reduce_sum(log_sum_exp(log_probs))\n",
    "    else:\n",
    "        return -tf.reduce_sum(log_sum_exp(log_probs),[1,2])\n",
    "\n",
    "\n",
    "def sample_from_discretized_mix_logistic(l,nr_mix):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src, sr = librosa.load(\"voice.wav\", sr=16000)\n",
    "src = src.reshape(1, -1, 1)\n",
    "src_out = np.random.randn(1,30,src.shape[1])\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, None, 1))\n",
    "y_hat = tf.placeholder(tf.float32, shape=(None, 30, None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l = tf.transpose(y_hat, perm=(0,2,1))\n",
    "xs = tf.shape(x)\n",
    "ls = tf.shape(l)\n",
    "nr_mix = tf.cast(ls[-1] / 3, tf.int32) # here and below: unpacking the params of the mixture of logistics\n",
    "\n",
    "logit_probs = l[:,:,:nr_mix]\n",
    "means = l[:, :, nr_mix:2 * nr_mix]\n",
    "log_scales = tf.max(l[:, :, 2 * nr_mix:3 * nr_mix], min=-7.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12814, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(means, feed_dict={x:src, y_hat:src_out})\n",
    "    \n",
    "    print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    \"\"\" numerically stable log_sum_exp implementation that prevents overflow \"\"\"\n",
    "    # TF ordering\n",
    "    axis = len(x.size()) - 1\n",
    "    m, _ = torch.max(x, dim=axis)\n",
    "    m2, _ = torch.max(x, dim=axis, keepdim=True)\n",
    "    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n",
    "\n",
    "\n",
    "def discretized_mix_logistic_loss_torch(y_hat, y, num_classes=256,\n",
    "                                  log_scale_min=-7.0, reduce=True):\n",
    "    \"\"\"Discretized mixture of logistic distributions loss\n",
    "\n",
    "    Note that it is assumed that input is scaled to [-1, 1].\n",
    "\n",
    "    Args:\n",
    "        y_hat (Variable): Predicted output (B x C x T)\n",
    "        y (Variable): Target (B x T x 1).\n",
    "        num_classes (int): Number of classes\n",
    "        log_scale_min (float): Log scale minimum value\n",
    "        reduce (bool): If True, the losses are averaged or summed for each\n",
    "          minibatch.\n",
    "\n",
    "    Returns\n",
    "        Variable: loss\n",
    "    \"\"\"\n",
    "    assert y_hat.dim() == 3\n",
    "    assert y_hat.size(1) % 3 == 0\n",
    "    nr_mix = y_hat.size(1) // 3\n",
    "\n",
    "    # (B x T x C)\n",
    "    y_hat = y_hat.transpose(1, 2)\n",
    "\n",
    "    # unpack parameters. (B, T, num_mixtures) x 3\n",
    "    logit_probs = y_hat[:, :, :nr_mix]\n",
    "    means = y_hat[:, :, nr_mix:2 * nr_mix]\n",
    "    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n",
    "\n",
    "    # B x T x 1 -> B x T x num_mixtures\n",
    "    y = y.expand_as(means)\n",
    "\n",
    "    centered_y = y - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n",
    "    cdf_plus = F.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n",
    "    cdf_min = F.sigmoid(min_in)\n",
    "\n",
    "    # log probability for edge case of 0 (before scaling)\n",
    "    # equivalent: torch.log(F.sigmoid(plus_in))\n",
    "    log_cdf_plus = plus_in - F.softplus(plus_in)\n",
    "\n",
    "    # log probability for edge case of 255 (before scaling)\n",
    "    # equivalent: (1 - F.sigmoid(min_in)).log()\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "\n",
    "    # probability for all other cases\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "\n",
    "    mid_in = inv_stdv * centered_y\n",
    "    # log probability in the center of the bin, to be used in extreme cases\n",
    "    # (not actually used in our code)\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n",
    "\n",
    "    # tf equivalent\n",
    "    \"\"\"\n",
    "    log_probs = tf.where(x < -0.999, log_cdf_plus,\n",
    "                         tf.where(x > 0.999, log_one_minus_cdf_min,\n",
    "                                  tf.where(cdf_delta > 1e-5,\n",
    "                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n",
    "                                           log_pdf_mid - np.log(127.5))))\n",
    "    \"\"\"\n",
    "    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n",
    "    # for num_classes=65536 case? 1e-7? not sure..\n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "\n",
    "    inner_inner_out = inner_inner_cond * \\\n",
    "        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n",
    "        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n",
    "    inner_cond = (y > 0.999).float()\n",
    "    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond = (y < -0.999).float()\n",
    "    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "\n",
    "    log_probs = torch.sum(log_probs, dim=-1, keepdim=True) + F.log_softmax(logit_probs, -1)\n",
    "\n",
    "    if reduce:\n",
    "        return -torch.sum(log_sum_exp(log_probs))\n",
    "    else:\n",
    "        return -log_sum_exp(log_probs).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def to_one_hot(tensor, n, fill_with=1.):\n",
    "    # we perform one hot encore with respect to the last axis\n",
    "    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n",
    "    if tensor.is_cuda:\n",
    "        one_hot = one_hot.cuda()\n",
    "    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n",
    "    return Variable(one_hot)\n",
    "\n",
    "\n",
    "def sample_from_discretized_mix_logistic_torch(y, log_scale_min=-7.0):\n",
    "    \"\"\"\n",
    "    Sample from discretized mixture of logistic distributions\n",
    "\n",
    "    Args:\n",
    "        y (Variable): B x C x T\n",
    "        log_scale_min (float): Log scale minimum value\n",
    "\n",
    "    Returns:\n",
    "        Variable: sample in range of [-1, 1].\n",
    "    \"\"\"\n",
    "    assert y.size(1) % 3 == 0\n",
    "    nr_mix = y.size(1) // 3\n",
    "\n",
    "    # B x T x C\n",
    "    y = y.transpose(1, 2)\n",
    "    logit_probs = y[:, :, :nr_mix]\n",
    "\n",
    "    # sample mixture indicator from softmax\n",
    "    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n",
    "    temp = logit_probs.data - torch.log(- torch.log(temp))\n",
    "    _, argmax = temp.max(dim=-1)\n",
    "\n",
    "    # (B, T) -> (B, T, nr_mix)\n",
    "    one_hot = to_one_hot(argmax, nr_mix)\n",
    "    # select logistic parameters\n",
    "    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n",
    "    log_scales = torch.clamp(torch.sum(\n",
    "        y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n",
    "    # sample from logistic & clip to interval\n",
    "    # we don't actually round to the nearest 8bit value when sampling\n",
    "    u = Variable(means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5))\n",
    "    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n",
    "\n",
    "    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mixture():\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    x, sr = librosa.load(\"voice.wav\", sr=16000)\n",
    "    assert sr == 16000\n",
    "\n",
    "    T = len(x)\n",
    "    x = x.reshape(1, T, 1)\n",
    "    y = Variable(torch.from_numpy(x)).float()\n",
    "    y_hat = Variable(torch.rand(1, 30, T)).float()\n",
    "\n",
    "    print(y.size(), y_hat.size())\n",
    "\n",
    "    loss = discretized_mix_logistic_loss_torch(y_hat, y)\n",
    "    print(loss)\n",
    "\n",
    "    loss = discretized_mix_logistic_loss_torch(y_hat, y, reduce=False)\n",
    "    print(loss.size(), y.size())\n",
    "    assert loss.size() == y.size()\n",
    "\n",
    "    y = sample_from_discretized_mix_logistic_torch(y_hat)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12814, 1]) torch.Size([1, 30, 12814])\n",
      "Variable containing:\n",
      "1.00000e+05 *\n",
      "  8.6754\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "torch.Size([1, 12814, 1]) torch.Size([1, 12814, 1])\n",
      "torch.Size([1, 12814])\n"
     ]
    }
   ],
   "source": [
    "test_mixture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
